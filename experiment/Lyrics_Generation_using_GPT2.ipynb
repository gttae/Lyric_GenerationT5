{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dIIVWEyM-Zd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "import sacrebleu\n",
        "from tqdm.auto import tqdm\n",
        "import csv\n",
        "\n",
        "# 반주 어휘 파일을 로드하는 함수\n",
        "def load_instrumental_vocab(vocab_file):\n",
        "    with open(vocab_file, 'r', encoding='utf-8') as file:\n",
        "        vocab = {line.strip(): i for i, line in enumerate(file.readlines())}\n",
        "    return vocab\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, vocab, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.vocab = vocab  # 반주 어휘\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        instrumental_data = eval(item['instrumental'])  # 리스트 형태로 저장된 문자열을 리스트로 변환\n",
        "        lyrics_data = item['lyrics']\n",
        "\n",
        "        # 반주 데이터를 어휘 인덱스로 변환\n",
        "        instrumental_tokens = [str(self.vocab[token]) if token in self.vocab else '0' for token in instrumental_data]\n",
        "        instrumental_str = ' '.join(instrumental_tokens)\n",
        "\n",
        "        input_text = f\"instrumental: {instrumental_str} lyrics:\"\n",
        "        target_text = lyrics_data\n",
        "\n",
        "        input_encodings = self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        target_encodings = self.tokenizer(target_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = input_encodings.input_ids.squeeze()\n",
        "        target_ids = target_encodings.input_ids.squeeze()\n",
        "\n",
        "        labels = torch.cat((input_ids, target_ids), dim=0)\n",
        "        labels = labels[:self.max_length]  # 최대 길이로 자르기\n",
        "        input_ids = input_ids[:self.max_length]\n",
        "\n",
        "        return input_ids, labels\n",
        "\n",
        "# 데이터 로드 및 분할\n",
        "vocab_file_path = '/content/drive/MyDrive/vocab_instrumental.vocab'  # 어휘 파일 경로 수정 필요\n",
        "instrumental_vocab = load_instrumental_vocab(vocab_file_path)\n",
        "\n",
        "df = pd.read_parquet('/content/drive/MyDrive/dataset_chords.parquet')  # 파일 경로 수정 필요\n",
        "train_size = int(0.9 * len(df))\n",
        "val_size = len(df) - train_size\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
        "\n",
        "dataset = MidiDataset(tokenizer, df, instrumental_vocab)\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# 입력과 타겟 예시 출력\n",
        "first_batch = next(iter(train_loader))\n",
        "source_example, target_example = first_batch[0][0], first_batch[1][0]\n",
        "\n",
        "source_text = tokenizer.decode(source_example, skip_special_tokens=True)\n",
        "target_text = tokenizer.decode(target_example, skip_special_tokens=True)\n",
        "\n",
        "print(\"입력 예시:\", source_text)\n",
        "print(\"타겟 예시:\", target_text)\n",
        "\n",
        "# 모델 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 학습\n",
        "model.train()\n",
        "for epoch in range(50):  # 학습 에포크\n",
        "    for input_ids, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 검증 및 BLEU 점수 계산\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "        input_ids = input_ids.to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=512,\n",
        "            num_beams=5,\n",
        "            repetition_penalty=2.0,\n",
        "            no_repeat_ngram_size=4\n",
        "        )\n",
        "        pred_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        true_text = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        predictions.extend(pred_text)\n",
        "        actuals.extend([true_text])\n",
        "\n",
        "# BLEU-2gram 스코어 구하기\n",
        "bleu_2_score = sacrebleu.corpus_bleu(predictions, actuals, weights=(0.5, 0.5)).score\n",
        "\n",
        "# BLEU-3gram 스코어 구하기\n",
        "bleu_3_score = sacrebleu.corpus_bleu(predictions, actuals, weights=(0.33, 0.33, 0.33)).score\n",
        "\n",
        "print(f\"BLEU-2 Score: {bleu_2_score}\")\n",
        "print(f\"BLEU-3 Score: {bleu_3_score}\")\n",
        "\n",
        "# 결과 저장\n",
        "with open('/content/drive/MyDrive/lyrics_predictions_gpt2.csv', 'w', newline='', encoding='utf-8') as csvfile:  # 파일 경로 수정 필요\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Predicted Lyrics', 'Actual Lyrics'])\n",
        "    for pred, actual in zip(predictions, actuals):\n",
        "        writer.writerow([pred, actual[0]])\n",
        "\n",
        "# 모델 저장\n",
        "model.save_pretrained('/content/drive/MyDrive/saved_model_gpt2')  # 파일 경로 수정 필요\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/saved_tokenizer_gpt2')  # 파일 경로 수정 필요\n"
      ]
    }
  ]
}