{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H3QZBYRK0S_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "import sacrebleu\n",
        "from tqdm.auto import tqdm\n",
        "import csv\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        instrumental_data = item['instrumental']\n",
        "        lyrics_data = item['lyrics']\n",
        "        source = f\"instrumental: {instrumental_data}\"\n",
        "        target = f\"lyrics: {lyrics_data}\"\n",
        "\n",
        "        source_encodings = self.tokenizer(source, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        target_encodings = self.tokenizer(target, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        return source_encodings.input_ids.squeeze(), target_encodings.input_ids.squeeze()\n",
        "\n",
        "# 데이터 로드 및 분할\n",
        "df = pd.read_parquet('/content/drive/MyDrive/cleaned_dataset_t5_2.parquet')\n",
        "train_size = int(0.9 * len(df))\n",
        "val_size = len(df) - train_size\n",
        "\n",
        "# Use BART tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "dataset = LyricsDataset(tokenizer, df)\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# 모델 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Use BART model\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 학습\n",
        "model.train()\n",
        "for epoch in range(50):  # 학습 에포크\n",
        "    for source, target in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=source, labels=target)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 검증 및 BLEU 점수 계산\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "with torch.no_grad():\n",
        "    for source, target in tqdm(val_loader, desc=\"Validating\"):\n",
        "        source = source.to(device)\n",
        "        outputs = model.generate(input_ids=source, max_length=512, num_beams=5)\n",
        "        pred_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        true_text = tokenizer.batch_decode(target, skip_special_tokens=True)\n",
        "\n",
        "        predictions.extend(pred_text)\n",
        "        actuals.extend([true_text])\n",
        "\n",
        "# BLEU-2gram 스코어 구하기\n",
        "bleu_2_score = sacrebleu.corpus_bleu(predictions, actuals, weights=(0.5, 0.5)).score\n",
        "\n",
        "# BLEU-3gram 스코어 구하기\n",
        "bleu_3_score = sacrebleu.corpus_bleu(predictions, actuals, weights=(0.33, 0.33, 0.33)).score\n",
        "\n",
        "print(f\"BLEU-2 Score: {bleu_2_score}\")\n",
        "print(f\"BLEU-3 Score: {bleu_3_score}\")\n",
        "\n",
        "# 결과 저장\n",
        "with open('/content/drive/MyDrive/lyrics_predictions.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Predicted Lyrics', 'Actual Lyrics'])\n",
        "    for pred, actual in zip(predictions, actuals):\n",
        "        writer.writerow([pred, actual[0]])\n",
        "\n",
        "\n",
        "# 모델 저장\n",
        "model.save_pretrained('/content/drive/MyDrive/saved_model_bart')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/saved_tokenizer_bart')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
